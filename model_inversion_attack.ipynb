{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36007494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/research/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09902c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2e7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ea3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "\n",
    "base_llm = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48267e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_model_name = \"facebook/bart-base\"\n",
    "\n",
    "inv_tokenizer = AutoTokenizer.from_pretrained(inv_model_name)\n",
    "\n",
    "inv_base_model = BartForConditionalGeneration.from_pretrained(inv_model_name, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76fb992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_init_weight is None - skipping mu_head.weight initialization\n",
      "mu_init_bias is None - skipping mu_head.bias initialization\n",
      "logvar_init_weight is None - skipping logvar_head.weight initialization\n",
      "logvar_init_bias is None - skipping logvar_head.bias initialization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train.sgt_model import SGTModel\n",
    "\n",
    "sgt = SGTModel(1536, 8, 2, 1, None, None, None, None).to(device)\n",
    "\n",
    "sgt.load_state_dict(torch.load('/home/alex/research/stained-glass-transform-pytorch/train/checkpoints/best_sgt.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "987314a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(torch.nn.Module):\n",
    "    def __init__(self, sgt, base_llm):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sgt = sgt\n",
    "        self.base_llm = base_llm\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        embeds = self.base_llm.model.embed_tokens(input_ids)\n",
    "        embeds, _, _ = self.sgt.sample(embeds, attention_mask=attention_mask)\n",
    "\n",
    "        return self.base_llm(inputs_embeds=embeds, attention_mask=attention_mask, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2001bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "class InversionModel(torch.nn.Module):\n",
    "    def __init__(self, model, tokenizer, input_features_d):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "        self.proj = torch.nn.Linear(input_features_d, model.model.encoder.embed_tokens.embedding_dim)\n",
    "\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=4,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_2_SEQ_LM\",\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "        )\n",
    "\n",
    "        self.model = get_peft_model(self.model, lora_cfg)\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "    def forward(self, encoder_embeds, encoder_attention_mask, labels, **kwargs):\n",
    "        transformed_embeds = self.proj(encoder_embeds)\n",
    "\n",
    "        return self.model(\n",
    "            inputs_embeds=transformed_embeds, \n",
    "            attention_mask=encoder_attention_mask, \n",
    "            labels=labels,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def generate(self, encoder_embeds, encoder_attention_mask, **generation_kwargs):\n",
    "        transformed_embeds = self.proj(encoder_embeds)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                inputs_embeds=transformed_embeds,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                **generation_kwargs\n",
    "            )\n",
    "        \n",
    "        return generated_ids\n",
    "    \n",
    "    def generate_text(self, encoder_embeds, encoder_attention_mask, **generation_kwargs):\n",
    "        generated_ids = self.generate(encoder_embeds, encoder_attention_mask, **generation_kwargs)\n",
    "        \n",
    "        generated_texts = self.tokenizer.batch_decode(\n",
    "            generated_ids, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3566d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 442,368 || all params: 139,862,784 || trainable%: 0.3163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = LLM(sgt, base_llm).eval().to(device)\n",
    "inv_model = InversionModel(inv_base_model, inv_tokenizer, 1536).eval().to(device)\n",
    "inv_model.load_state_dict(torch.load('/home/alex/research/latest.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7b7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Daniel Kids has 1000$ on his bank account\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68e2fab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[40586, 22522,   702,   220,    16,    15,    15,    15,     3,   389,\n",
       "           806,  6073,  2692]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf5900cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = llm_tokenizer(text, return_tensors='pt')\n",
    "\n",
    "outputs = llm(**tokenized.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83fe5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = inv_model.generate_text(\n",
    "    encoder_embeds=outputs.hidden_states[1], \n",
    "    encoder_attention_mask=tokenized['attention_mask'],\n",
    "    max_length=32,\n",
    "    do_sample=True,\n",
    "    temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6186d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dillon Kids have a bank account issue ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb844b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
