{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193deaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/research/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5c0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd35a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Fix for models without pad token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736446bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:01<00:00, 105457.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "DATASET_SIZE = 400\n",
    "\n",
    "dataset = datasets.load_dataset('ag_news')\n",
    "\n",
    "dataset = [item['text'] for item in tqdm(dataset['train'])]\n",
    "\n",
    "dataset = dataset[:DATASET_SIZE]\n",
    "\n",
    "class SGTDataset(Dataset):\n",
    "    \"\"\"Custom dataset for SGT training\"\"\"\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f133cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import ModelOutput\n",
    "\n",
    "class ObfOutput(ModelOutput):\n",
    "    loss: torch.Tensor\n",
    "    # optional: surface individual losses to logs via callbacks if wanted\n",
    "    mi_loss: torch.Tensor\n",
    "    utility_loss: torch.Tensor\n",
    "    abs_cos_loss: torch.Tensor\n",
    "    norm_loss: torch.Tensor\n",
    "    obfuscations_loss: torch.Tensor\n",
    "\n",
    "class ObfuscationModel(nn.Module):\n",
    "    def __init__(self, llm, sgt, sgt_loss):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "        self.sgt = sgt\n",
    "        self.sgt_loss = sgt_loss\n",
    "\n",
    "        # Freeze LLM parameters\n",
    "        for p in self.llm.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        self.last_metrics = {}\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            embeds = self.llm.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        B = embeds.size(0)\n",
    "        \n",
    "        if B % 2 != 0:\n",
    "            embeds = embeds[:-1]\n",
    "            B = B - 1\n",
    "        \n",
    "        if B < 2:\n",
    "            loss = torch.tensor(0.0, device=embeds.device, requires_grad=True)\n",
    "            return loss\n",
    "        \n",
    "        B_half = B // 2\n",
    "        x = embeds[:B_half]\n",
    "        x_independent = embeds[B_half:B_half*2]\n",
    "\n",
    "        attention_mask = attention_mask[:B_half]\n",
    "\n",
    "        loss_dict = self.sgt_loss(x, x_independent, self.llm, self.sgt, attention_mask)\n",
    "        loss = loss_dict['total_loss']\n",
    "\n",
    "        if loss < 0.2:\n",
    "            self.sgt_loss.set_alpha('alpha_cos', 1.0)\n",
    "            self.sgt_loss.set_alpha('alpha_obfuscation', 1.0)\n",
    "\n",
    "        self.last_metrics = {\n",
    "            \"fuck\": 12.0,\n",
    "            'train/fucking': 1.0,\n",
    "            'eval/fuc': 2.0\n",
    "        }\n",
    "        # self.log({\n",
    "        #     # Primary losses\n",
    "        #     \"total_loss\": loss.item(),\n",
    "        #     \"obfuscations_loss\": loss_dict['obfuscations_loss'].item(),\n",
    "            \n",
    "        #     # Raw component losses\n",
    "        #     \"raw/utility_loss\": loss_dict['utility_loss'].item(),\n",
    "        #     \"raw/mi_loss\": loss_dict['mi_loss'].item(),\n",
    "        #     \"raw/abs_cos_loss\": loss_dict['abs_cos_loss'].item(),\n",
    "        #     \"raw/norm_loss\": loss_dict['norm_loss'].item(),\n",
    "            \n",
    "        #     # Scaled (weighted) component losses\n",
    "        #     \"scaled/utility_loss\": loss_dict['scaled_utility_loss'].item(),\n",
    "        #     \"scaled/mi_loss\": loss_dict['scaled_mi_loss'].item(),\n",
    "        #     \"scaled/cos_loss\": loss_dict['scaled_cos_loss'].item(),\n",
    "        #     \"scaled/norm_loss\": loss_dict['scaled_norm_loss'].item(),\n",
    "        # })\n",
    "        \n",
    "        return (loss, )\n",
    "\n",
    "    def sgt_forward(self, x):\n",
    "        return self.sgt(x)\n",
    "    \n",
    "    def sgt_sample(self, x):\n",
    "        return self.sgt.sample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d53458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGT(nn.Module):\n",
    "    def __init__(self, d, nhead=8, ff=4, layers=1):\n",
    "        super().__init__()\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=ff*d, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=layers)\n",
    "        self.mu_head = nn.Linear(d, d)\n",
    "        self.logvar_head = nn.Linear(d, d)\n",
    "\n",
    "        nn.init.zeros_(self.mu_head.weight)\n",
    "        nn.init.zeros_(self.mu_head.bias)\n",
    "        nn.init.zeros_(self.logvar_head.weight)\n",
    "        nn.init.constant_(self.logvar_head.bias, -5.0) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_embeds = self.enc(x)\n",
    "\n",
    "        mu = self.mu_head(hidden_embeds)\n",
    "        logvar = self.logvar_head(hidden_embeds)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def sample(self, x):\n",
    "        mu, logvar = self(x)\n",
    "\n",
    "        logvar = torch.clamp(logvar, min=-10, max=2)\n",
    "        \n",
    "        eps = torch.randn_like(mu)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = x + mu + eps * torch.exp(0.5 * logvar)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61cc4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgt = SGT(d=llm.config.hidden_size, nhead=8, ff=2, layers=1)\n",
    "sgt = sgt.to(llm.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08792ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import SGTLoss\n",
    "\n",
    "sgt_loss = SGTLoss(\n",
    "    embedding_weights=llm.model.embed_tokens.weight,\n",
    "    alpha_mi=0.0,\n",
    "    alpha_cos=1.0,\n",
    "    alpha_norm=0.01,\n",
    "\n",
    "    alpha_utility=1.0,\n",
    "    alpha_obfuscation=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70aa03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SGTDataset(dataset, tokenizer, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a8a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObfuscationModel(llm, sgt, sgt_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d9a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMetricsCallback(TrainerCallback):\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        # Clear model metrics at the start of each step\n",
    "        if hasattr(self.model, 'last_metrics'):\n",
    "            self.model.last_metrics = {}\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # This is called when trainer is about to log\n",
    "        if logs is not None and hasattr(self.model, 'last_metrics'):\n",
    "            # Add your metrics to the logs dict BEFORE they go to TensorBoard\n",
    "            logs.update(self.model.last_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a012e939",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomMetricsCallback' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     21\u001b[39m metrics_callback = CustomMetricsCallback()\n\u001b[32m     23\u001b[39m trainer = Trainer(\n\u001b[32m     24\u001b[39m     model=model,\n\u001b[32m     25\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     callbacks=[metrics_callback]\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/transformers/trainer.py:2572\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2569\u001b[39m     steps_trained_progress_bar = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % args.gradient_accumulation_steps == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_step_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# We explicitly want to avoid relying on `accelerator.accumulate` for generation training\u001b[39;00m\n\u001b[32m   2575\u001b[39m context = (\n\u001b[32m   2576\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2580\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/transformers/trainer_callback.py:522\u001b[39m, in \u001b[36mCallbackHandler.on_step_begin\u001b[39m\u001b[34m(self, args, state, control)\u001b[39m\n\u001b[32m    520\u001b[39m control.should_evaluate = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    521\u001b[39m control.should_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_step_begin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/transformers/trainer_callback.py:556\u001b[39m, in \u001b[36mCallbackHandler.call_event\u001b[39m\u001b[34m(self, event, args, state, control, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, **kwargs):\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m         result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[32m    569\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mCustomMetricsCallback.on_step_begin\u001b[39m\u001b[34m(self, args, state, control, **kwargs)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_step_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, state, control, **kwargs):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Clear model metrics at the start of each step\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mlast_metrics\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      5\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.last_metrics = {}\n",
      "\u001b[31mAttributeError\u001b[39m: 'CustomMetricsCallback' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sgt_model\",\n",
    "    per_device_train_batch_size=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1000,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    dataloader_drop_last=True,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    per_device_eval_batch_size=8,\n",
    ")\n",
    "\n",
    "metrics_callback = CustomMetricsCallback()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=torch.utils.data.Subset(train_dataset, list(range(64))),\n",
    "    callbacks=[metrics_callback]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703faeda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
