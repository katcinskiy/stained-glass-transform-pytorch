# @package _global_
loss:
  alpha_utility: 1.0
  alpha_obfuscation: 1.0
  alpha_logvar_mse: 0.0
  alpha_abs_cos: 0.5

training:
  apply_gradient_clipping: true
  lr: 5e-5

model:
  llm_name: "Qwen/Qwen2.5-1.5B"
  tokenizer_max_length: 16
  cache_clean_logits: False
  sgt:
    nhead: 8
    ff: 2
    layers: 1
    mu_init_weight: 0
    mu_init_bias: 0
    logvar_init_weight: 0
    logvar_init_bias: -5

data:
  dataset_name: "ag_news"
  num_samples: 50000
