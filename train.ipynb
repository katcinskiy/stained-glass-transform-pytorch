{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5091c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b dev https://github.com/katcinskiy/stained-glass-transform-pytorch\n",
    "!cd stained-glass-transform-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5d6f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/research/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022be4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4663ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/gemma-2-2b\"  # or \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Fix for models without pad token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0737bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/research/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0813 13:34:29.048000 26981 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer('Heya', return_tensors='pt')\n",
    "\n",
    "tokenized = {k: v.to(device) for k, v in tokenized.items()}\n",
    "\n",
    "res = llm.generate(**tokenized, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858123fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Heya!\\n\\nI'm back with another drawing!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids=res[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd872373",
   "metadata": {},
   "source": [
    "# SGT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbe05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGT(nn.Module):\n",
    "    def __init__(self, d, nhead=8, ff=4, layers=1):\n",
    "        super().__init__()\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=ff*d, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=layers)\n",
    "        self.mu_head = nn.Linear(d, d)\n",
    "        self.logvar_head = nn.Linear(d, d)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_embeds = self.enc(x)\n",
    "\n",
    "        mu = self.mu_head(hidden_embeds)\n",
    "        logvar = self.logvar_head(hidden_embeds)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def sample(self, x):\n",
    "        mu, logvar = self(x)\n",
    "        eps = torch.randn_like(mu)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = x + mu + eps * torch.exp(0.5 * logvar)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0a648",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3a067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:01<00:00, 105951.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "DATASET_SIZE = 400\n",
    "\n",
    "dataset = datasets.load_dataset('ag_news')\n",
    "\n",
    "dataset = [item['text'] for item in tqdm(dataset['train'])]\n",
    "\n",
    "dataset = dataset[:DATASET_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78bec661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGTDataset(Dataset):\n",
    "    \"\"\"Custom dataset for SGT training\"\"\"\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568d8d5",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c2c89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGTLoss(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            embedding_weights,\n",
    "            alpha_mi,\n",
    "            alpha_cos,\n",
    "            alpha_norm,\n",
    "            alpha_utility,\n",
    "            alpha_obfuscation\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.embeddings_norm = embedding_weights.norm(dim=-1).detach().median()\n",
    "        self.alpha_mi = alpha_mi\n",
    "        self.alpha_cos = alpha_cos\n",
    "        self.alpha_norm = alpha_norm\n",
    "        self.alpha_utility = alpha_utility\n",
    "        self.alpha_obfuscation = alpha_obfuscation\n",
    "    \n",
    "    def forward(self, x, x_independent, llm, sgt):\n",
    "        x_tilde, mu, logvar = sgt.sample(x)\n",
    "\n",
    "        utility_loss = self._utility_loss(llm, x, x_tilde)\n",
    "\n",
    "        mi_loss = self._mi_loss(x, x_independent, x_tilde, logvar, sgt)\n",
    "        abs_cos_loss = self._abs_cos_loss(x, x_tilde)\n",
    "        norm_loss = self._median_norm_penalty(x, mu)\n",
    "\n",
    "        obfuscations_loss = self.alpha_mi * mi_loss + self.alpha_cos * abs_cos_loss + self.alpha_norm * norm_loss\n",
    "\n",
    "        total_loss = self.alpha_utility * utility_loss + self.alpha_obfuscation * obfuscations_loss\n",
    "\n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'obfuscations_loss': obfuscations_loss,\n",
    "            'utility_loss': utility_loss,\n",
    "            'mi_loss': mi_loss,\n",
    "            'abs_cos_loss': abs_cos_loss,\n",
    "            'norm_loss': norm_loss\n",
    "        }\n",
    "\n",
    "\n",
    "    def _abs_cos_loss(self, x, x_tilde):\n",
    "        cos_sim = F.cosine_similarity(x, x_tilde, dim=-1) # shape (b, l, d)\n",
    "        return cos_sim.abs().mean()\n",
    "    \n",
    "    def _utility_loss(self, llm, x, x_tilde):\n",
    "        with torch.no_grad():\n",
    "            logits_clean = llm(inputs_embeds=x).logits\n",
    "        \n",
    "        logits_obf = llm(inputs_embeds=x_tilde).logits\n",
    "\n",
    "        x_probas = F.softmax(logits_clean, dim=-1)\n",
    "        \n",
    "        x_tilde_log_probas = F.log_softmax(logits_obf, dim=-1)\n",
    "        ce_loss = (-x_probas * x_tilde_log_probas).sum(dim=-1).mean()\n",
    "        return ce_loss\n",
    "    \n",
    "    def _mi_loss(self, x, x_independent, x_tilde, logvar, sgt):\n",
    "\n",
    "        mu_independent, logvar_independent = sgt(x_independent)\n",
    "        \n",
    "        # 1. Log determinant ratio\n",
    "\n",
    "        # log_det_ratio = (logvar_l - logvar_i).sum(dim=(-1, -2)) # it was from chatgpt\n",
    "        #TODO: check here, why logvar is a vector? do we need torch.linalg.det here?? why we have sum here? можем ли мы считать, что у нас матрица всегда диагональная? мы можем из СГТ всегда возвращать диагональную матрицу! итого это будут векторы\n",
    "        log_det_ratio = (logvar - logvar_independent).sum(dim=(-1, -2))\n",
    "        \n",
    "        # 2. Махаланобис\n",
    "        mahalanobis_distance = self._mahalanobis(x_tilde, x_independent, mu_independent, logvar_independent)\n",
    "        \n",
    "        # MI loss - среднее по батчу\n",
    "        return (log_det_ratio + mahalanobis_distance).mean()\n",
    "    \n",
    "    def _mahalanobis(self, x_tilde, x_independent, mu_independent, logvar_independent):\n",
    "        # TODO: test it\n",
    "\n",
    "        vector_in_norm = (x_tilde - x_independent - mu_independent)\n",
    "\n",
    "        logvar_independent_inverse = torch.exp(-logvar_independent)\n",
    "        \n",
    "        mahalanobis_distance = ((vector_in_norm ** 2) * logvar_independent_inverse).sum(dim=(-1, -2))\n",
    "\n",
    "        return mahalanobis_distance\n",
    "\n",
    "    def _median_norm_penalty(self, x, mu):\n",
    "        norms = (x + mu).norm(dim=-1)\n",
    "        \n",
    "        penalty = (norms.mean() - self.embeddings_norm).abs()\n",
    "\n",
    "        return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df0e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgt_loss = SGTLoss(\n",
    "    embedding_weights=llm.model.embed_tokens.weight,\n",
    "    alpha_mi=1,\n",
    "    alpha_cos=1,\n",
    "    alpha_norm=1,\n",
    "    alpha_utility=1,\n",
    "    alpha_obfuscation=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d88f38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObfuscationTrainer(Trainer):\n",
    "    \"\"\"Custom trainer for SGT with frozen LLM\"\"\"\n",
    "    def __init__(self, sgt, llm, tokenizer, sgt_loss, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sgt = sgt\n",
    "        self.llm = llm\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sgt_loss = sgt_loss\n",
    "        \n",
    "        # Freeze LLM parameters\n",
    "        for p in self.llm.parameters():\n",
    "            p.requires_grad_(False)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, num_items_in_batch, return_outputs=False):\n",
    "\n",
    "        # print(inputs)\n",
    "\n",
    "        toks = inputs[\"input_ids\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeds = self.llm.get_input_embeddings()(toks)\n",
    "        \n",
    "        B = embeds.size(0)\n",
    "        \n",
    "        if B % 2 != 0:\n",
    "            embeds = embeds[:-1]\n",
    "            B = B - 1\n",
    "        \n",
    "        if B < 2:\n",
    "            # Skip if batch too small\n",
    "            loss = torch.tensor(0.0, device=embeds.device, requires_grad=True)\n",
    "            return (loss, {\"loss\": loss}) if return_outputs else loss\n",
    "        \n",
    "        # Split batch for unbiased MI estimation\n",
    "        B_half = B // 2\n",
    "        x = embeds[:B_half]\n",
    "        x_independent = embeds[B_half:B_half*2]\n",
    "\n",
    "        loss_dict = self.sgt_loss(x, x_independent, self.llm, model)\n",
    "        loss = loss_dict['total_loss']\n",
    "\n",
    "        self.log({\n",
    "                \"train/total_loss\": loss.item(),\n",
    "                \"train/obfuscations_loss\": loss_dict['obfuscations_loss'].item(),\n",
    "                \"train/utility_loss\": loss_dict['utility_loss'].item(),\n",
    "                \"train/mi_loss\": loss_dict['mi_loss'].item(),\n",
    "                \"train/abs_cos_loss\": loss_dict['abs_cos_loss'].item(),\n",
    "                \"train/norm_loss\": loss_dict['norm_loss'].item(),\n",
    "        })\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b789c45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 61.94 MiB is free. Including non-PyTorch memory, this process has 11.05 GiB memory in use. Of the allocated memory 10.75 GiB is allocated by PyTorch, with 70.00 MiB allocated in private pools (e.g., CUDA Graphs), and 39.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     25\u001b[39m trainer = ObfuscationTrainer(\n\u001b[32m     26\u001b[39m     model=sgt,\n\u001b[32m     27\u001b[39m     sgt=sgt,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     train_dataset=train_dataset\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/transformers/trainer.py:2582\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2575\u001b[39m context = (\n\u001b[32m   2576\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2580\u001b[39m )\n\u001b[32m   2581\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2582\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2585\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2586\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2587\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2588\u001b[39m ):\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2590\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/transformers/trainer.py:3796\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3795\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3796\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3798\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3800\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3801\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3802\u001b[39m ):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mObfuscationTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch, return_outputs)\u001b[39m\n\u001b[32m     36\u001b[39m x = embeds[:B_half]\n\u001b[32m     37\u001b[39m x_independent = embeds[B_half:B_half*\u001b[32m2\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m loss_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msgt_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_independent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m loss = loss_dict[\u001b[33m'\u001b[39m\u001b[33mtotal_loss\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m.log({\n\u001b[32m     43\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrain/total_loss\u001b[39m\u001b[33m\"\u001b[39m: loss.item(),\n\u001b[32m     44\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrain/obfuscations_loss\u001b[39m\u001b[33m\"\u001b[39m: loss_dict[\u001b[33m'\u001b[39m\u001b[33mobfuscations_loss\u001b[39m\u001b[33m'\u001b[39m].item(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrain/norm_loss\u001b[39m\u001b[33m\"\u001b[39m: loss_dict[\u001b[33m'\u001b[39m\u001b[33mnorm_loss\u001b[39m\u001b[33m'\u001b[39m].item(),\n\u001b[32m     49\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mSGTLoss.forward\u001b[39m\u001b[34m(self, x, x_independent, llm, sgt)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_independent, llm, sgt):\n\u001b[32m     20\u001b[39m     x_tilde, mu, logvar = sgt.sample(x)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     utility_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_utility_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tilde\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     mi_loss = \u001b[38;5;28mself\u001b[39m._mi_loss(x, x_independent, x_tilde, logvar, sgt)\n\u001b[32m     25\u001b[39m     abs_cos_loss = \u001b[38;5;28mself\u001b[39m._abs_cos_loss(x, x_tilde)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mSGTLoss._utility_loss\u001b[39m\u001b[34m(self, llm, x, x_tilde)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_utility_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm, x, x_tilde):\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         logits_clean = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.logits\n\u001b[32m     50\u001b[39m     logits_obf = llm(inputs_embeds=x_tilde).logits\n\u001b[32m     52\u001b[39m     x_probas = F.softmax(logits_clean, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:959\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    958\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    961\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/.venv/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py:566\u001b[39m, in \u001b[36mGemma2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states[:, slice_indices, :])\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.final_logit_softcapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     logits = \u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinal_logit_softcapping\u001b[49m\n\u001b[32m    567\u001b[39m     logits = torch.tanh(logits)\n\u001b[32m    568\u001b[39m     logits = logits * \u001b[38;5;28mself\u001b[39m.config.final_logit_softcapping\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 61.94 MiB is free. Including non-PyTorch memory, this process has 11.05 GiB memory in use. Of the allocated memory 10.75 GiB is allocated by PyTorch, with 70.00 MiB allocated in private pools (e.g., CUDA Graphs), and 39.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "sgt = SGT(d=llm.config.hidden_size, nhead=8, ff=4, layers=2)\n",
    "sgt = sgt.to(llm.device)\n",
    "\n",
    "train_dataset = SGTDataset(dataset, tokenizer, max_length=128)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sgt_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",  # or \"tensorboard\"\n",
    "    dataloader_drop_last=True,  # Ensure consistent batch sizes\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ObfuscationTrainer(\n",
    "    model=sgt,\n",
    "    sgt=sgt,\n",
    "    llm=llm,\n",
    "    tokenizer=tokenizer,\n",
    "    sgt_loss=sgt_loss,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbc797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
